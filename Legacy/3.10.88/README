HOWTO: Enable SR-IOV with  Mellanox OFED driver on upstream 3.10.88 kernel for
Linux VM on Hyper-V and Azure environment.

1. Follow the below instructions for SR-IOV to work on 3.10.88 kernel

1.1 Checkout the upstream 3.10.88 kernel
https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git/commit/?h=2ec142700ec00142cdcd8c6529ad77cb9ec7a026

1.2 Apply the 2 patches
0001-hack-pci-res-code-to-make-OFED-driver-work.patch
0002-vmbus-backport-for-3.10.88.patch

1.3 Use the "kernel-config" as the .config and build & install the kernel/drivers.

1.4 If you're on a local Hyper-V host, add a Mellanox ConnectX-3 VF to the VM and "lspci" should show
0002:00:02.0 Network controller: Mellanox Technologies MT27500/MT27520 Family [ConnectX-3/ConnectX-3 Pro Virtual Function]

If your VM is on Azure, follow this link to enable Accelerated Networking (i.e. SR-IOV for network)
https://docs.microsoft.com/en-us/azure/virtual-network/create-vm-accelerated-networking-cli

Kernel 3.10.88 inbox mellanox driver does not support SR-IOV, please use the OFED driver.

2. Steps to enable the OFED driver

2.1 Download the latest OFED driver from:
http://www.mellanox.com/page/products_dyn?product_family=26&mtag=linux_sw_drivers
*select related distribution.

2.2 Uncompress the src/MLNX_OFED_SRC-4.2-1.2.0.0.tgz, and run
src/MLNX_OFED_SRC-4.2-1.2.0.0/install.pl. After installing the necessary
prerequisite packages, we'll start to build the OFED drivers and this can take ~30
minutes, and finally the generated drivers will be installed automatically with
no error.

2.3 Reboot the VM, and the MLX OFED driver should load fine.

[root@localhost ~]# lspci -s 0002:00:02.0 -vvv
0002:00:02.0 Network controller: Mellanox Technologies MT27500/MT27520 Family [ConnectX-3/ConnectX-3 Pro Virtual Function]
        Subsystem: Mellanox Technologies Device 61b0
        Control: I/O- Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr- Stepping- SERR- FastB2B- DisINTx-
        Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
        Latency: 0
        NUMA node: 0
        Region 2: Memory at fe0000000 (64-bit, prefetchable) [size=8M]
        Capabilities: [60] Express (v2) Endpoint, MSI 00
                DevCap: MaxPayload 512 bytes, PhantFunc 0, Latency L0s <64ns, L1 <1us
                        ExtTag- AttnBtn- AttnInd- PwrInd- RBE- FLReset+ SlotPowerLimit 0.000W
                DevCtl: Report errors: Correctable- Non-Fatal- Fatal- Unsupported-
                        RlxdOrd- ExtTag- PhantFunc- AuxPwr- NoSnoop- FLReset-
                        MaxPayload 128 bytes, MaxReadReq 128 bytes
                DevSta: CorrErr- UncorrErr- FatalErr- UnsuppReq- AuxPwr- TransPend-
                LnkCap: Port #0, Speed 8GT/s, Width x8, ASPM not supported, Exit Latency L0s <64ns, L1 <1us
                        ClockPM- Surprise- LLActRep- BwNot- ASPMOptComp-
                LnkCtl: ASPM Disabled; RCB 64 bytes Disabled- CommClk-
                        ExtSynch- ClockPM- AutWidDis- BWInt- AutBWInt-
                LnkSta: Speed unknown, Width x0, TrErr- Train- SlotClk- DLActive- BWMgmt- ABWMgmt-
                DevCap2: Completion Timeout: Range ABCD, TimeoutDis+, LTR-, OBFF Not Supported
                DevCtl2: Completion Timeout: 50us to 50ms, TimeoutDis-, LTR-, OBFF Disabled
                LnkSta2: Current De-emphasis Level: -6dB, EqualizationComplete-, EqualizationPhase1-
                         EqualizationPhase2-, EqualizationPhase3-, LinkEqualizationRequest-
        Capabilities: [9c] MSI-X: Enable+ Count=108 Masked-
                Vector table: BAR=2 offset=00002000
                PBA: BAR=2 offset=00003000
        Capabilities: [40] Power Management version 0
                Flags: PMEClk- DSI- D1- D2- AuxCurrent=0mA PME(D0-,D1-,D2-,D3hot-,D3cold-)
                Status: D0 NoSoftRst- PME-Enable- DSel=0 DScale=0 PME-
        Kernel driver in use: mlx4_core
        Kernel modules: mlx4_core


[root@localhost ~]# dmesg |grep -i mlx
[    6.809576] mlx4_core: Mellanox ConnectX core driver v4.2-1.2.0
[    6.810379] mlx4_core: Initializing 0002:00:02.0
[    6.815065] mlx4_core 0002:00:02.0: Detected virtual function - running in slave mode
[    6.815091] mlx4_core 0002:00:02.0: Sending reset
[    6.815178] mlx4_core 0002:00:02.0: Sending vhcr0
[    6.816572] mlx4_core 0002:00:02.0: HCA minimum page size:512
[    6.817144] mlx4_core 0002:00:02.0: Timestamping is not supported in slave mode
[    6.817144] mlx4_core: device is working in RoCE mode: Unknown
[    6.817145] mlx4_core: gid_type 0 for UD QPs is not supported by the device, gid_type 3 was chosen instead
[    6.817145] mlx4_core: UD QP Gid type is: Unknown
[    6.820040] mlx4_core 0002:00:02.0: irq 40 for MSI/MSI-X
[    6.820149] mlx4_core 0002:00:02.0: irq 41 for MSI/MSI-X
[    6.820257] mlx4_core 0002:00:02.0: irq 42 for MSI/MSI-X
[    6.820365] mlx4_core 0002:00:02.0: irq 43 for MSI/MSI-X
[    6.820473] mlx4_core 0002:00:02.0: irq 44 for MSI/MSI-X
[    6.820581] mlx4_core 0002:00:02.0: irq 45 for MSI/MSI-X
[    6.820689] mlx4_core 0002:00:02.0: irq 46 for MSI/MSI-X
[    6.820796] mlx4_core 0002:00:02.0: irq 47 for MSI/MSI-X
[    6.820904] mlx4_core 0002:00:02.0: irq 48 for MSI/MSI-X
[    6.890827] mlx4_en: Mellanox ConnectX HCA Ethernet driver v4.2-1.2.0
[    6.892064] mlx4_en 0002:00:02.0: Activating port:1
[    6.910195] mlx4_en: 0002:00:02.0: Port 1: Using 8 TX rings
[    6.911328] mlx4_en: 0002:00:02.0: Port 1: Using 8 RX rings
[    6.912107] mlx4_en: 0002:00:02.0: Port 1: Initializing port
[    6.913279] mlx4_core 0002:00:02.0 eth2: joined to eth1
[   13.585232] <mlx4_ib> mlx4_ib_add: mlx4_ib: Mellanox ConnectX InfiniBand driver v4.2-1.2.0
[   13.587272] <mlx4_ib> mlx4_ib_add: counter index 10 for port 1 allocated 1
[   13.588473] ib_query_pkey failed (-1) for mlx4_0 (index 0)
[   13.589632] mlx4_core 0002:00:02.0: mlx4_ib: multi-function enabled
[   13.590248] mlx4_core 0002:00:02.0: mlx4_ib: operating in qp1 tunnel mode
[   15.177138] mlx4_en: enP2p0s2: Steering Mode 2
[   15.199897] mlx4_en: enP2p0s2: Link Up

Verify:

Check for activity on the VF using "ethtool -S eth0 | grep vf_" command. If you see output such as below, SR-IOV (accelerated networking)
is working.

vf_rx_packets: 992956
vf_rx_bytes: 2749784180
vf_tx_packets: 2656684
vf_tx_bytes: 1099443970
vf_tx_dropped: 0

Note: Hyper-V: the patchset backported the transparent SR-IOV VF patch, so we should set IP address to
the ethX interface and not the VF interface.
